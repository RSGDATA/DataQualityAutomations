{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4024fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ab3fb",
   "metadata": {},
   "source": [
    "# This section  below is for the purpose of tabularizing json and performing data validation from source to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc6f1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"MainId\": 1111,\n",
    "            \"firstName\": \"Sherlock\",\n",
    "            \"lastName\": \"Homes\",\n",
    "            \"categories\": [\n",
    "                {\n",
    "                    \"CategoryID\": 1,\n",
    "                    \"CategoryName\": \"Example\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"MainId\": 122,\n",
    "            \"firstName\": \"James\",\n",
    "            \"lastName\": \"Watson\",\n",
    "            \"categories\": [\n",
    "                {\n",
    "                    \"CategoryID\": 2,\n",
    "                    \"CategoryName\": \"Example2\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"messages\": [], \n",
    "    \"success\": 'True' \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0695a",
   "metadata": {},
   "source": [
    "## Function That Parses JSON And Creates A DataFrame To Be Tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43132351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the outer level meta data to input as a parameter in the json_normalize() function\n",
    "def outer_flatten_list_with_df(dictionary):\n",
    "    most_outer_list = []\n",
    "    most_outer_df = pd.json_normalize(dictionary).T\n",
    "    most_outer_df['NodePath'] = most_outer_df.index\n",
    "    most_outer_list = []\n",
    "    df_list = []\n",
    "    droplist = []\n",
    "\n",
    "    for i in range(len(most_outer_df)):\n",
    "        ttype = most_outer_df.iloc[i, 0]\n",
    "        if isinstance(ttype, dict) == True:\n",
    "            most_outer_list.append(most_outer_df.index[i])\n",
    "            droplist.append(most_outer_df.index[i])\n",
    "        elif isinstance(ttype, list) == True:\n",
    "            most_outer_list.append(most_outer_df.index[i])\n",
    "            droplist.append(most_outer_df.index[i])\n",
    "        elif isinstance(ttype, list) == True and len(ttype) == 0:\n",
    "            inner_df = most_outer_df['NodePath'].iloc[i]\n",
    "            inner_df['NodePath'].iloc[i] = key + '.' + inner_df\n",
    "        elif isinstance(ttype, list) == True and len(ttype) == 0:\n",
    "            inner_df['NodePath'].iloc[i] = key + '.' + inner_df['NodePath'].iloc[i]\n",
    "            \n",
    "    only_outer_df = most_outer_df.drop(droplist)\n",
    "    df_list.append(only_outer_df)\n",
    "    return [df_list, most_outer_list]\n",
    "\n",
    "# Takes meta data from outer_flatten_list_with_df and parses one level down\n",
    "def inner_flatten_list_with_df1(lst, dictionary):\n",
    "    inner_layers_list = []\n",
    "    df_list = []\n",
    "    droplist = []\n",
    "\n",
    "    if len(lst) == 0:\n",
    "        return 'No values to parse'\n",
    "\n",
    "    for key in lst:\n",
    "        droplist = []\n",
    "        if '.' in key:\n",
    "            keylist = key.split('.')\n",
    "            inner_df = pd.json_normalize(dictionary, record_path=keylist).T\n",
    "            inner_df['NodePath'] = inner_df.index\n",
    "        else:\n",
    "            inner_df = pd.json_normalize(dictionary, record_path=key).T\n",
    "            inner_df['NodePath'] = inner_df.index\n",
    "\n",
    "        for i in range(len(inner_df)):\n",
    "            ttype = inner_df.iloc[i, 0]\n",
    "            if isinstance(ttype, dict) == True:\n",
    "                # mol.append(most_outer_df.index[i])\n",
    "                inner_layers_list.append(key + '.' + inner_df.index[i])\n",
    "                droplist.append(inner_df.index[i])\n",
    "            elif isinstance(ttype, list) == True:\n",
    "                # mol.append(most_outer_df.index[i])\n",
    "                inner_layers_list.append(key + '.' + inner_df.index[i])\n",
    "                droplist.append(inner_df.index[i])\n",
    "            elif isinstance(ttype, list) == True and len(ttype) == 0:\n",
    "                inner_df['NodePath'].iloc[i] = key + '.' + inner_df['NodePath'].iloc[i]\n",
    "            elif isinstance(ttype, list) == True and len(ttype) == 0:\n",
    "                inner_df['NodePath'].iloc[i] = key + '.' + inner_df['NodePath'].iloc[i]\n",
    "            else:\n",
    "                inner_df['NodePath'].iloc[i] = key + '.' + inner_df['NodePath'].iloc[i]\n",
    "\n",
    "        only_outer_df = inner_df.drop(droplist)\n",
    "        df_list.append(only_outer_df)\n",
    "\n",
    "    return [df_list, inner_layers_list]\n",
    "\n",
    "\n",
    "# Takes meta data from outer_flatten_list_with_df and parses one level down\n",
    "def inner_flatten_list_with_df2(lst, dictionary):\n",
    "    inner_layers_list = []\n",
    "    df_list = []\n",
    "    droplist = []\n",
    "\n",
    "    if len(lst) == 0:\n",
    "        return 'No values to parse'\n",
    "\n",
    "    for key in lst:\n",
    "        droplist = []\n",
    "        if '.' in key:\n",
    "            keylist = key.split('.')\n",
    "            inner_df = pd.json_normalize(dictionary, record_path=keylist).T\n",
    "            inner_df['NodePath'] = inner_df.index\n",
    "        else:\n",
    "            inner_df = pd.json_normalize(dictionary, record_path=key).T\n",
    "            inner_df['NodePath'] = inner_df.index\n",
    "\n",
    "        for i in range(len(inner_df)):\n",
    "            ttype = inner_df.iloc[i, 0]\n",
    "            if isinstance(ttype, dict) == True:\n",
    "                # mol.append(most_outer_df.index[i])\n",
    "                inner_layers_list.append(key + '.' + inner_df.index[i])\n",
    "                droplist.append(inner_df.index[i])\n",
    "            elif isinstance(ttype, list) == True:\n",
    "                # mol.append(most_outer_df.index[i])\n",
    "                inner_layers_list.append(key + '.' + inner_df.index[i])\n",
    "                droplist.append(inner_df.index[i])\n",
    "            elif isinstance(ttype, list) == True and len(ttype) == 0:\n",
    "                inner_df['NodePath'].iloc[i] = key + '.' + inner_df['NodePath'].iloc[i]\n",
    "            elif isinstance(ttype, list) == True and len(ttype) == 0:\n",
    "                inner_df['NodePath'].iloc[i] = key + '.' + inner_df['NodePath'].iloc[i]\n",
    "            else:\n",
    "                inner_df['NodePath'].iloc[i] = key + '.' + inner_df['NodePath'].iloc[i]\n",
    "\n",
    "        only_outer_df = inner_df.drop(droplist)\n",
    "        df_list.append(only_outer_df)\n",
    "\n",
    "    return [df_list, inner_layers_list]\n",
    "\n",
    "\n",
    "# Uses the three functions and creates a pattern that will parse down to the lowest level in a given JSON.\n",
    "def json_dataset(dictionary):\n",
    "    all_meta_data = []\n",
    "    nested_list_of_dataframes = []\n",
    "\n",
    "    outer_list = outer_flatten_list_with_df(dictionary)[1]\n",
    "    outer_df = outer_flatten_list_with_df(dictionary)[0]\n",
    "\n",
    "    all_meta_data.append(outer_list)\n",
    "    nested_list_of_dataframes.append(outer_df)\n",
    "\n",
    "    inner_flatten_list = inner_flatten_list_with_df1(outer_list, dictionary)[1]\n",
    "    inner_flatten_df1 = inner_flatten_list_with_df1(outer_list, dictionary)[0]\n",
    "\n",
    "    all_meta_data.append(inner_flatten_list)\n",
    "    nested_list_of_dataframes.append(inner_flatten_df1)\n",
    "\n",
    "    while len(inner_flatten_list) > 0:\n",
    "        inner_flatten_list = inner_flatten_list_with_df2(inner_flatten_list, dictionary)[1]\n",
    "        inner_flatten_df2 = inner_flatten_list_with_df2(inner_flatten_list, dictionary)[0]\n",
    "        all_meta_data.append(inner_flatten_list)\n",
    "        nested_list_of_dataframes.append(inner_flatten_df2)\n",
    "\n",
    "    # if len(inner_flatten_list) > 0:\n",
    "    #     inner_flatten_list = inner_flatten1(inner_flatten_list, dictionary)\n",
    "    #     all_meta_data.append(inner_flatten_list)\n",
    "\n",
    "    json_dataframe_list = []\n",
    "    for dflist in nested_list_of_dataframes:\n",
    "        for df in dflist:\n",
    "            json_dataframe_list.append(df)\n",
    "\n",
    "    deal_concatdf = pd.concat(json_dataframe_list[:-1]).sort_values(by='NodePath')\n",
    "    sorted_json_concatdf = deal_concatdf.rename(columns={0: \"Values\"})\n",
    "    json_concatdf = sorted_json_concatdf[['NodePath', 'Values', 1]]\n",
    "\n",
    "    return json_concatdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15eb8037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                 NodePath    Values       1\n",
       " MainId        data.MainId      1111     122\n",
       " firstName  data.firstName  Sherlock   James\n",
       " lastName    data.lastName     Homes  Watson\n",
       " success           success      True     NaN]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[json_dataset(json)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07da025",
   "metadata": {},
   "source": [
    "## Test every Combination From Source To Target And Bring Back ids OF Passes and Fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "157b5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_id_list2 = []\n",
    "snowflake_id_list2 = []\n",
    "match_list = []\n",
    "sql_mismatch_list = []\n",
    "snowflake_mismatch_list = []\n",
    "mismatch_list = []\n",
    "\n",
    "# This code is testing if the JSON has matching property name and loan number. Then it puts the IDs from source and target\n",
    "# into separate lists, where the IDs are the same index between the two lists.\n",
    "for i in range(len(parsed_json_sql_dataframe_list)):\n",
    "    for j in range(len(parsed_json_snowflake_dataframe_list)):\n",
    "        # if parsed_json_snowflake_dataframe_list[j].query('NodePath == \"propertyName\"').iloc[0, 0] \\\n",
    "        # == parsed_json_sql_dataframe_list[i].query('NodePath == \"propertyName\"').iloc[0, 0] and \\\n",
    "        # parsed_json_snowflake_dataframe_list[j].iloc[:, 1] == parsed_json_sql_dataframe_list[i].iloc[:, 1]:\n",
    "\n",
    "        if len(parsed_json_snowflake_dataframe_list[j]) == len(parsed_json_sql_dataframe_list[i]) \\\n",
    "           and len(set(parsed_json_snowflake_dataframe_list[j].index) - set(parsed_json_sql_dataframe_list[i].index)) == 0 \\\n",
    "           and len(set(parsed_json_snowflake_dataframe_list[j].columns) - set(parsed_json_sql_dataframe_list[i].columns)) == 0:\n",
    "            \n",
    "            results = parsed_json_snowflake_dataframe_list[j].compare(parsed_json_sql_dataframe_list[i])\n",
    "\n",
    "            # If the tests are a pass, then show the IDs from the table\n",
    "            if len(results) == 0:\n",
    "                sql_id_list2.append(sql_source_dataframe.iloc[i, 0])\n",
    "                snowflake_id_list2.append(snowflake_target_dataframe.iloc[j, 4])\n",
    "                match_list.append('Pass')\n",
    "            if len(results) > 0:\n",
    "                sql_mismatch_list.append(sql_source_dataframe.iloc[i, 0])\n",
    "                snowflake_mismatch_list.append(snowflake_target_dataframe.iloc[j, 4])\n",
    "                mismatch_list.append('Fail')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757d04e",
   "metadata": {},
   "source": [
    "# Make Dataset Of All Test Combinations And Write Results To A File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba6adc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame for mismatches\n",
    "mismatch_df = pd.DataFrame({\n",
    "    'SQL_id_Column_Values': sql_mismatch_list,\n",
    "    'Snowflake_INPUT_KEY_Column_Values': snowflake_mismatch_list,\n",
    "    'Results': mismatch_list\n",
    "})\n",
    "\n",
    "# Create DataFrame for matches\n",
    "match_df = pd.DataFrame({\n",
    "    'SQL_id_Column_Values': sql_id_list2,\n",
    "    'Snowflake_INPUT_KEY_Column_Values': snowflake_id_list2,\n",
    "    'Results': match_list\n",
    "})\n",
    "\n",
    "# Combine match and mismatch DataFrames\n",
    "cmm_individual_results_df = pd.concat([mismatch_df, match_df])\n",
    "cmm_individual_results_df.index = np.arange(1, len(cmm_individual_results_df) + 1)\n",
    "\n",
    "# Export the combined DataFrame to an Excel file\n",
    "cmm_individual_results_df.to_excel('outbound/Cmm_Individual_Results.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2a0b8",
   "metadata": {},
   "source": [
    "# Data Set Showing The Results By Unique Key For Snowflake Snowflake_Input_Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbd817d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and display unique 'Pass' results by Snowflake_DEAL_INPUT_KEY_Column_Values\n",
    "unique_pass_df = cmm_individual_results_df.query('Results == \"Pass\"').drop_duplicates('Snowflake_INPUT_KEY_Column_Values')\n",
    "unique_pass_df.index = np.arange(1, len(unique_pass_df) + 1)\n",
    "unique_pass_df\n",
    "\n",
    "# Filter and display unique 'Fail' results by Snowflake_DEAL_INPUT_KEY_Column_Values\n",
    "unique_fail_df = cmm_individual_results_df.query('Results == \"Fail\"').drop_duplicates('Snowflake_INPUT_KEY_Column_Values')\n",
    "unique_fail_df.index = np.arange(1, len(unique_fail_df) + 1)\n",
    "unique_fail_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198baa56",
   "metadata": {},
   "source": [
    "## Get Ids Of JSON That Has Matching PropertyName and Loanid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ab2a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_id_list = []\n",
    "snowflake_id_list = []\n",
    "\n",
    "# only use the results from this code when the amount of loans are 1:1 from SQL to Snowflake.\n",
    "\n",
    "# This code is testing if the JSON has matching property name and loan number. Then it puts the IDs from source and target\n",
    "# into separate lists, where the IDs are the same index between the two lists.\n",
    "for i in range(len(parsed_json_snowflake_dataframe_list)):\n",
    "    for j in range(len(parsed_json_sql_dataframe_list)):\n",
    "        if parsed_json_snowflake_dataframe_list[i].query('NodePath == \"propertyName\"').iloc[0, 0] \\\n",
    "           == parsed_json_sql_dataframe_list[j].query('NodePath == \"propertyName\"').iloc[0, 0] and \\\n",
    "           parsed_json_snowflake_dataframe_list[i].iloc[i, 1] \\\n",
    "           == parsed_json_sql_dataframe_list[j].iloc[j, 1]:\n",
    "            \n",
    "            sql_id_list.append(sql_source_dataframe.iloc[j, 0])\n",
    "            snowflake_id_list.append(snowflake_target_dataframe.iloc[i, -4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd871a51",
   "metadata": {},
   "source": [
    "# Query The SQL And Snowflake Tables For JSON By Id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c24ca858",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sql_dataframe_list = []\n",
    "concat_snowflake_dataframe_list = []\n",
    "\n",
    "# Loops through the ID lists in parallel. The JSONs that are on the same rows of the IDs should match.\n",
    "# The corresponding JSONs are then parsed and put in a list. The matching dataframes are now in the same indices\n",
    "# between the two lists of dataframes. This means when they are concatenated, all JSON elements will be in corresponding\n",
    "# indices.\n",
    "\n",
    "for sql_id, snowflake_id in zip(sql_id_list, snowflake_id_list):\n",
    "    concat_sql_dataframe_list.append(json_dataset(turn_json_into_dictionary(sql_source_dataframe.query('id == @sql_id').iloc[0])))\n",
    "    concat_snowflake_dataframe_list.append(json_dataset(json.loads(snowflake_target_dataframe.query('INPUT_KEY == @snowflake_id').iloc[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f6250",
   "metadata": {},
   "source": [
    "# Concatenates The SQL DataFrame List Into One DataFrame Of Parsed JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "232e31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes concatenated DataFrame, and adds an index column that will match snowflake concatenated dataframe.\n",
    "sql_concat_df = pd.concat(concat_sql_dataframe_list).reset_index(drop=True).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15562288",
   "metadata": {},
   "source": [
    "# Concatenates The Snowfalke DataFrame List Into One DataFrame Of Parsed JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02042ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes concatenated DataFrame, and adds an index column that will match sql concatenated dataframe.\n",
    "snowflake_concat_df = pd.concat(concat_snowflake_dataframe_list).reset_index(drop=True).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a104a62",
   "metadata": {},
   "source": [
    "# The Section Below is for the purpose of sorting all columns in independently and adding a new index to each. This results in the data being trully 1:1 between environments when used in the compare_function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ce9521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_columns(source_df, target_df):\n",
    "    compare_result_numbers = []\n",
    "    source_columns = []\n",
    "    target_columns = []\n",
    "    compare_result_dfs = []\n",
    "\n",
    "    for source_column, target_column in zip(source_df.columns, target_df.columns):\n",
    "        sort_source_df = source_df[source_column].to_frame().sort_values(by=source_column)\n",
    "        sort_target_df = target_df[target_column].to_frame().sort_values(by=target_column)\n",
    "\n",
    "        aligned_source = sort_source_df.reset_index(drop=True)\n",
    "        aligned_target = sort_target_df.reset_index(drop=True)\n",
    "\n",
    "        results = aligned_source[source_column].compare(aligned_target[target_column])\n",
    "\n",
    "        source_columns.append(source_column)\n",
    "        target_columns.append(target_column)\n",
    "        compare_result_numbers.append(len(results))\n",
    "        compare_result_dfs.append(results)\n",
    "\n",
    "    return [compare_result_numbers, compare_result_dfs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8172e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_result_numbers = []\n",
    "source_columns = []\n",
    "target_columns = []\n",
    "compare_result_dfs = []\n",
    "\n",
    "full_sorted_df_source = pd.DataFrame()\n",
    "full_sorted_df_target = pd.DataFrame()\n",
    "\n",
    "for source_column, target_column in zip(dbsource.columns, datahubtarget.columns):\n",
    "    sort_four_columns_source = dbsource[source_column].to_frame().sort_values(by=source_column)\n",
    "    sort_four_columns_target = datahubtarget[target_column].to_frame().sort_values(by=target_column)\n",
    "\n",
    "    aligned_source = sort_four_columns_source.reset_index(drop=True)\n",
    "    aligned_target = sort_four_columns_target.reset_index(drop=True)\n",
    "\n",
    "    full_sorted_df_source = pd.concat([full_sorted_df_source, aligned_source], axis=1)\n",
    "    full_sorted_df_target = pd.concat([full_sorted_df_target, aligned_target], axis=1)\n",
    "\n",
    "    # for key, value in columns_to_convert.items():\n",
    "    #     if key == aligned_source.columns:\n",
    "    #         print(source_column)\n",
    "\n",
    "    #     aligned_source[key] = pd.to_numeric(aligned_source[key], errors='coerce')\n",
    "    #     aligned_target[key] = pd.to_numeric(aligned_target[key], errors='coerce')\n",
    "\n",
    "    results = aligned_source[source_column].compare(aligned_target[target_column])\n",
    "\n",
    "    source_columns.append(source_column)\n",
    "    target_columns.append(target_column)\n",
    "    compare_result_numbers.append(len(results))\n",
    "    compare_result_dfs.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f74a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame({\n",
    "    'Source Columns': source_columns,\n",
    "    'Target Columns': target_columns,\n",
    "    'Results': compare_result_numbers\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86754f",
   "metadata": {},
   "source": [
    "# This section looks for file in a file path, uses the string from the file name to construct meta data to populate sql scripts and dataframes to then be processed and validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4164a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming 'filepath' is defined elsewhere\n",
    "# plist = os.listdir(filepath)\n",
    "\n",
    "letterdict = {}\n",
    "for file in plist:\n",
    "    if '.json' in file:\n",
    "        letter_list = re.findall(r'(\\d+_historical)', file)\n",
    "        letter_result = '_'.join(map(str, letter_list))\n",
    "\n",
    "        gdict = {}\n",
    "        for file in plist:\n",
    "            if 'historical_2024081683200734.csv.gz' in file:\n",
    "                notincluded = []\n",
    "                data = pd.read_csv(file, sep=\",\")\n",
    "                letter_result = file.split(\"_\")[1].replace(\"Historical\", \"\")\n",
    "                result = re.findall(r'\\d+', letter_result)\n",
    "                gdict[int(result[0])] = data\n",
    "\n",
    "        sorted_gdict = sorted(gdict.items())\n",
    "\n",
    "grouped_data = defaultdict(list)\n",
    "columnlist = []\n",
    "schemalist = []\n",
    "tablelist = []\n",
    "testkeylist = []\n",
    "filelist = []\n",
    "column_specification_list = []\n",
    "count = 1\n",
    "\n",
    "for k, v in enumerate(list(letterdict.keys())):\n",
    "    sortkeys = letterdict[k]\n",
    "    print(sortkeys)\n",
    "\n",
    "    most_rec = sorted_gdict[1][1]\n",
    "    nrd = pd.read_csv(r'filepath'.format(most_rec), nrows=2)\n",
    "    column_specification_count = 1\n",
    "\n",
    "    stg_dict = {}\n",
    "\n",
    "    for column in nrd.columns:\n",
    "        for k in keys:\n",
    "            if k == 'Name':\n",
    "                stg_dict[k] = column\n",
    "            elif k == 'sequence':\n",
    "                stg_dict[k] = str(column_specification_count)\n",
    "            elif k == 'role' and column_specification_count == 1:\n",
    "                stg_dict[k] = 'Key'\n",
    "                column_specification_count += 1\n",
    "            else:\n",
    "                stg_dict[k] = 'Value'\n",
    "                column_specification_count += 1\n",
    "\n",
    "        stg_json_dump = json.dumps(stg_dict)\n",
    "        column_specification_list.append(stg_json_dump)\n",
    "\n",
    "    tablelist.append(k)\n",
    "    columnlist.append(list(nrd.columns))\n",
    "    schemalist.append('CreditLens')\n",
    "    testkeylist.append(k)\n",
    "    filelist.append(most_rec)\n",
    "    count += 1\n",
    "\n",
    "for key, value in zip(tablelist, column_specification_list):\n",
    "    grouped_data[key].append(value)\n",
    "\n",
    "for table in tablelist:\n",
    "    csllist.append(', '.join(grouped_data[table]))\n",
    "\n",
    "# Metadata script template, the column name and column will have the ordinal position of the new column\n",
    "# to be tested in the target table after final job run.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
